# app/routes/chat_v1.py
from fastapi import APIRouter, HTTPException, Query
from backend.app.models import PromptRequest, PromptResponse
from backend.app.services.ollama_service import call_ollama
from backend.app.services.rag_service import rag_query
from backend.app.services.memory_service_redis import generate_session_id
from backend.app.services.retriever_service import RetrieverConfig
from backend.app.logger import logger
from backend.app.services.tts_service import call_tts
import traceback

router = APIRouter()

# å›ºå®š Prompt æ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯å“ˆåˆ©æ³¢ç‰¹ï¼Œè¯·ç”¨é­”æ³•å¸ˆçš„è¯­æ°”ã€‚
ç”¨æˆ·é—®é¢˜å¦‚ä¸‹ï¼š
{question}
è¯·æ ¹æ®å·²æœ‰çŸ¥è¯†ç»™å‡ºç®€æ´æ¸…æ™°çš„å›ç­”ã€‚
"""

@router.post("/chat_v1", response_model=PromptResponse)
def chat_with_model(
    data: PromptRequest,
    # session_id: str = Query(None, description="ä¼šè¯IDï¼Œå¯ç”±å¤–éƒ¨ç³»ç»Ÿä¼ å…¥ï¼ˆæœªä¼ åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰"),
):
    print(data)
    try:
        # # ä¼šè¯ç®¡ç†
        # # å¦‚æœ session_id æœªä¼ ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªæ–°çš„
        # if not session_id:
        #     session_id = generate_session_id()

        # æ¨¡å‹ å’Œ æ˜¯å¦è°ƒç”¨RAG
        model = "llama3"
        use_rag = False

        # æ‹¼æ¥ Prompt
        final_prompt = PROMPT_TEMPLATE.format(question=data.question)
        print(final_prompt)

        # è°ƒç”¨ LLM
        try:
            if use_rag:
                retriever_config = RetrieverConfig(k=3, score_threshold=0.5)
                answer = rag_query(
                    final_prompt,
                    model=model,
                    # session_id=session_id,
                    use_memory=False,
                    retriever_config=retriever_config
                )
            else:
                print("model: llama3")
                logger.info(f"Calling LLM with prompt: {final_prompt}")
                answer = call_ollama(final_prompt, model=model)
                print("æ–‡æœ¬æ¶ˆæ¯ç”Ÿæˆå®Œæ¯•")

            # logger.info("chat_request_received", prompt=final_prompt, use_rag=use_rag)

        except Exception as e:
            tb = traceback.format_exc()
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}\n{tb}")
            raise HTTPException(status_code=500, detail=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")


        # è°ƒç”¨ TTS
        audio_url = ""
        if data.isAudio:
            try:
                audio_url = call_tts(answer)
            except Exception as e:
                logger.error(f"TTS è°ƒç”¨å¤±è´¥: {str(e)}")
                # å¯ä»¥é€‰æ‹©ç»§ç»­è¿”å›æ–‡æœ¬ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥
                audio_url = ""

        return PromptResponse(response=answer, audio_url=audio_url)

    except Exception as e:
        tb = traceback.format_exc()
        logger.error(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}\n{tb}")
        raise HTTPException(status_code=500, detail=str(e))

# çº¯æ–‡æœ¬ç‰ˆ

# @router.post("/chat_v1", response_model=PromptResponse)
# def chat_with_model(
#     data: PromptRequest,
#     use_rag: bool = Query(False, description="æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“RAG"),
#     use_memory: bool = Query(False, description="æ˜¯å¦ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ï¼ˆä»…RAGæ¨¡å¼æœ‰æ•ˆï¼‰"),
#     session_id: str = Query(None, description="ä¼šè¯IDï¼Œå¯ç”±å¤–éƒ¨ç³»ç»Ÿä¼ å…¥ï¼ˆæœªä¼ åˆ™è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äºæµ‹è¯•ï¼‰"),
#     k: int = Query(3, description="æ£€ç´¢æ–‡æ¡£æ•°é‡"),
#     score_threshold: float = Query(None, description="ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)"),
#     filter_department: str = Query(None, description="å¯é€‰ï¼šéƒ¨é—¨è¿‡æ»¤ï¼Œä¾‹å¦‚ legal"),
# ):
#     try:
#         # å¦‚æœ session_id æœªä¼ ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªæµ‹è¯•ç”¨çš„
#         if not session_id:
#             session_id = generate_session_id()
#
#         if use_rag:
#             filter_metadata = {"department": filter_department} if filter_department else None
#             retriever_config = RetrieverConfig(k=k, score_threshold=score_threshold, filter_metadata=filter_metadata)
#
#             answer = rag_query(
#                 data.prompt,
#                 model=data.model,
#                 session_id=session_id,
#                 use_memory=use_memory,
#                 retriever_config=retriever_config
#             )
#         else:
#             answer = call_ollama(data.prompt, model=data.model)
#
#         logger.info("chat_request_received", prompt=data.prompt, use_rag=use_rag, retriever_config=retriever_config)
#
#         # # ğŸ”¹ è°ƒç”¨ TTSï¼Œå°†æ–‡å­—è½¬è¯­éŸ³
#         if isAudio:
#             audio_url = call_tts(answer)
#         else:
#             audio_url = None
#
#
#         return PromptResponse(response=answer, audio_url=audio_url)
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))
#


# 5. å·¥ä½œæµç¨‹
# 1.ç”¨æˆ·è°ƒç”¨ /chat
# å¦‚æœæ²¡ä¼  session_id â†’ è‡ªåŠ¨ç”Ÿæˆ UUID
# å¦‚æœ use_rag=True & use_memory=True â†’ å¸¦ä¸Šå†å²è®°å½•
#
# 2.rag_service.py æ£€ç´¢ FAISS ç»“æœ
#
# 3.æ‹¼æ¥ å†å² + çŸ¥è¯†åº“å†…å®¹ + é—®é¢˜
#
# 4.è°ƒç”¨ call_ollama
#
# 5.å¦‚æœ use_memory=True â†’ ä¿å­˜é—®ç­”åˆ° Redis
#
# 6.Redis è‡ªåŠ¨æ¸…ç†è¶…è¿‡ TTL çš„ä¼šè¯
